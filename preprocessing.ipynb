{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will preprocess a pair of documents in order to analyse their similarity afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "## dfSourceV2 = pd.read_csv('../Dados/v2_semeval-2022_task8_train-data_batch.csv')\n",
    "trainv1 = pd.read_csv('dados/train v0.1.csv')\n",
    "\n",
    "trainv1_enen = trainv1[(trainv1['url1_lang']=='en') & (trainv1['url2_lang']=='en')]\n",
    "\n",
    "trainv1_enen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar noticias a partir dos ids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that read json documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function reads a json file\n",
    "def readJsonFile(path):\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "    \n",
    "#Function returns a dataframe with the text of the pairs\n",
    "def getJsonDocumentPair(dataPath, pairId):\n",
    "    listIds = pairId.split('_')\n",
    "    doc1Id = listIds[0]\n",
    "    doc2Id = listIds[1]\n",
    "\n",
    "    doc1Path = dataPath + doc1Id[-2:] + '/' + doc1Id + '.json' \n",
    "    doc2Path = dataPath + doc2Id[-2:] + '/' + doc2Id + '.json' \n",
    "\n",
    "    doc1Json = readJsonFile(doc1Path)\n",
    "    doc2Json = readJsonFile(doc2Path)\n",
    "\n",
    "    return (doc1Json, doc2Json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting text to preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'dados/train v0.1/'\n",
    "#jsonPair = getJsonDocumentPair(dataPath,'1484084337_1484110209')\n",
    "#textDoc1 = jsonPair[0]['text']\n",
    "\n",
    "lista_docs = []\n",
    "lista_error = []\n",
    "lista_vazio = []\n",
    "values = trainv1_enen[['pair_id', 'Overall']]\n",
    "\n",
    "for index, values in values.iterrows():\n",
    "    try:\n",
    "        jsonPair = getJsonDocumentPair(dataPath, values['pair_id'])\n",
    "        textDoc1 = jsonPair[0]['text']\n",
    "        textDoc2 = jsonPair[1]['text']\n",
    "        if len(textDoc1)>0 and len(textDoc2)>0:\n",
    "            lista_docs.append((values['pair_id'], textDoc1, textDoc2, values['Overall']))\n",
    "        else:\n",
    "            lista_vazio.append(values['pair_id'])\n",
    "    except:\n",
    "        lista_error.append(values['pair_id'])\n",
    "\n",
    "#Creating DF to text\n",
    "#dfText = pd.DataFrame([[textDoc1]], columns=['original_text'])\n",
    "dfText = pd.DataFrame(lista_docs,  columns=['pair_id', 'doc1', 'doc2', 'Overall'])\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainv1_enen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_vazio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio do pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "other_punctuation = '—“”'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that removes punctuation \n",
    "def removePunctuation(text):\n",
    "    punctuationFreeDoc = \"\".join([i for i in text if i not in string.punctuation+other_punctuation])\n",
    "    return punctuationFreeDoc\n",
    "\n",
    "\n",
    "#Storing the puntuation free text\n",
    "dfText['clean_msg1']= dfText['doc1'].apply(lambda x:removePunctuation(x))\n",
    "dfText['clean_msg2']= dfText['doc2'].apply(lambda x:removePunctuation(x))\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['msg_lower1']= dfText['clean_msg1'].apply(lambda x: x.lower())\n",
    "dfText['msg_lower2']= dfText['clean_msg2'].apply(lambda x: x.lower())\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['msg_tokenized1']= dfText['msg_lower1'].apply(lambda x: nltk.word_tokenize(x))\n",
    "dfText['msg_tokenized2']= dfText['msg_lower2'].apply(lambda x: nltk.word_tokenize(x))\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talvez fazer o sentence tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('’')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(listWords):\n",
    "    filteredWords = [word for word in listWords if word not in stop_words]\n",
    "    return filteredWords\n",
    "\n",
    "dfText['no_stopwords1']= dfText['msg_tokenized1'].apply(lambda x: removeStopwords(x))\n",
    "dfText['no_stopwords2']= dfText['msg_tokenized2'].apply(lambda x: removeStopwords(x))\n",
    "dfText[['msg_tokenized1', 'no_stopwords1', 'msg_tokenized2', 'no_stopwords2']].head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem.porter import PorterStemmer\n",
    "#porter = PorterStemmer()\n",
    "\n",
    "#def stemming(listWords):\n",
    "#    stemText = [porter.stem(word) for word in listWords]\n",
    "#    return stemText\n",
    "\n",
    "#dfText['msg_stemmed1']= dfText['no_stopwords1'].apply(lambda x: stemming(x))\n",
    "#dfText['msg_stemmed2']= dfText['no_stopwords2'].apply(lambda x: stemming(x))\n",
    "#dfText[['no_stopwords1', 'msg_stemmed1', 'no_stopwords2', 'msg_stemmed2']].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(listWords):\n",
    "    lemmText = [wordnet_lemmatizer.lemmatize(word) for word in listWords]\n",
    "    return lemmText\n",
    "\n",
    "dfText['msg_lemmatized1']= dfText['no_stopwords1'].apply(lambda x: lemmatizer(x))\n",
    "dfText['msg_lemmatized2']= dfText['no_stopwords2'].apply(lambda x: lemmatizer(x))\n",
    "dfText[['no_stopwords1', 'msg_lemmatized1']].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação dos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard(word_tokens1, word_tokens2):\n",
    "\t# Combine both tokens to find union.\n",
    "\tboth_tokens = word_tokens1 + word_tokens2\n",
    "\tunion = set(both_tokens)\n",
    "\n",
    "\t# Calculate intersection.\n",
    "\tintersection = set()\n",
    "\tfor w in word_tokens1:\n",
    "\t\tif w in word_tokens2:\n",
    "\t\t\tintersection.add(w)\n",
    "\n",
    "\tif len(union)==0:\n",
    "\t\tjaccard_score = 0\n",
    "\telse:\n",
    "\t\tjaccard_score = len(intersection)/len(union)\n",
    "\treturn jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['jaccard'] = dfText.apply(lambda row: calculate_jaccard(row['msg_lemmatized1'], row['msg_lemmatized2']), axis=1)\n",
    "#calculate_jaccard(dfText['msg_lemmatized1'][0], dfText['msg_lemmatized2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['jaccard', 'Overall']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidf_similarity():\n",
    "\tvectorizer = TfidfVectorizer()\n",
    "\n",
    "\t# To make uniformed vectors, both documents need to be combined first.\n",
    "\tdocuments.insert(0, base_document)\n",
    "\tembeddings = vectorizer.fit_transform(documents)\n",
    "\n",
    "\tcosine_similarities = cosine_similarity(embeddings[0:1], embeddings[1:]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578d23e9265697bad3ff07bcff4c72684a1087c3ecac6cf42decc0cb53b76ed7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('data_science': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
