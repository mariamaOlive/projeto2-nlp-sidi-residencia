{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing - Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will preprocess a pair of documents in order to analyse their similarity afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "## dfSourceV2 = pd.read_csv('../Dados/v2_semeval-2022_task8_train-data_batch.csv')\n",
    "trainv1 = pd.read_csv('dados/train v0.1.csv')\n",
    "\n",
    "trainv1_enen = trainv1[(trainv1['url1_lang']=='en') & (trainv1['url2_lang']=='en')]\n",
    "\n",
    "trainv1_enen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar noticias a partir dos ids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that read json documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function reads a json file\n",
    "def readJsonFile(path):\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "    \n",
    "#Function returns a dataframe with the text of the pairs\n",
    "def getJsonDocumentPair(dataPath, pairId):\n",
    "    listIds = pairId.split('_')\n",
    "    doc1Id = listIds[0]\n",
    "    doc2Id = listIds[1]\n",
    "\n",
    "    doc1Path = dataPath + doc1Id[-2:] + '/' + doc1Id + '.json' \n",
    "    doc2Path = dataPath + doc2Id[-2:] + '/' + doc2Id + '.json' \n",
    "\n",
    "    doc1Json = readJsonFile(doc1Path)\n",
    "    doc2Json = readJsonFile(doc2Path)\n",
    "\n",
    "    return (doc1Json, doc2Json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting text to preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'dados/train v0.1/'\n",
    "#jsonPair = getJsonDocumentPair(dataPath,'1484084337_1484110209')\n",
    "#textDoc1 = jsonPair[0]['text']\n",
    "\n",
    "lista_docs = []\n",
    "lista_error = []\n",
    "lista_vazio = []\n",
    "values = trainv1_enen[['pair_id', 'Overall']]\n",
    "\n",
    "for index, values in values.iterrows():\n",
    "    try:\n",
    "        jsonPair = getJsonDocumentPair(dataPath, values['pair_id'])\n",
    "        textDoc1 = jsonPair[0]['text']\n",
    "        textDoc2 = jsonPair[1]['text']\n",
    "        if len(textDoc1)>0 and len(textDoc2)>0:\n",
    "            lista_docs.append((values['pair_id'], textDoc1, textDoc2, values['Overall']))\n",
    "        else:\n",
    "            lista_vazio.append(values['pair_id'])\n",
    "    except:\n",
    "        lista_error.append(values['pair_id'])\n",
    "\n",
    "#Creating DF to text\n",
    "#dfText = pd.DataFrame([[textDoc1]], columns=['original_text'])\n",
    "dfText = pd.DataFrame(lista_docs,  columns=['pair_id', 'doc1', 'doc2', 'Overall'])\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainv1_enen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_vazio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio do pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "other_punctuation = '—“”'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that removes punctuation \n",
    "def removePunctuation(text):\n",
    "    punctuationFreeDoc = \"\".join([i for i in text if i not in string.punctuation+other_punctuation])\n",
    "    return punctuationFreeDoc\n",
    "\n",
    "\n",
    "#Storing the puntuation free text\n",
    "dfText['clean_msg1']= dfText['doc1'].apply(lambda x:removePunctuation(x))\n",
    "dfText['clean_msg2']= dfText['doc2'].apply(lambda x:removePunctuation(x))\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['msg_lower1']= dfText['clean_msg1'].apply(lambda x: x.lower())\n",
    "dfText['msg_lower2']= dfText['clean_msg2'].apply(lambda x: x.lower())\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['msg_tokenized1']= dfText['msg_lower1'].apply(lambda x: nltk.word_tokenize(x))\n",
    "dfText['msg_tokenized2']= dfText['msg_lower2'].apply(lambda x: nltk.word_tokenize(x))\n",
    "dfText.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talvez fazer o sentence tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('’')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(listWords):\n",
    "    filteredWords = [word for word in listWords if word not in stop_words]\n",
    "    return filteredWords\n",
    "\n",
    "dfText['no_stopwords1']= dfText['msg_tokenized1'].apply(lambda x: removeStopwords(x))\n",
    "dfText['no_stopwords2']= dfText['msg_tokenized2'].apply(lambda x: removeStopwords(x))\n",
    "dfText[['msg_tokenized1', 'no_stopwords1', 'msg_tokenized2', 'no_stopwords2']].head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemming(listWords):\n",
    "    stemText = [porter.stem(word) for word in listWords]\n",
    "    return stemText\n",
    "\n",
    "dfText['msg_stemmed1']= dfText['no_stopwords1'].apply(lambda x: stemming(x))\n",
    "dfText['msg_stemmed2']= dfText['no_stopwords2'].apply(lambda x: stemming(x))\n",
    "dfText[['no_stopwords1', 'msg_stemmed1', 'no_stopwords2', 'msg_stemmed2']].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(listWords):\n",
    "    lemmText = [wordnet_lemmatizer.lemmatize(word) for word in listWords]\n",
    "    return lemmText\n",
    "\n",
    "dfText['msg_lemmatized1']= dfText['no_stopwords1'].apply(lambda x: lemmatizer(x))\n",
    "dfText['msg_lemmatized2']= dfText['no_stopwords2'].apply(lambda x: lemmatizer(x))\n",
    "dfText[['no_stopwords1', 'msg_lemmatized1']].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação dos algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_unique_docs(dfText, column1, column2):\n",
    "\n",
    "   data = []\n",
    "   for i in range(0, len(dfText)):\n",
    "      data.append(' '.join(dfText[column1][i]))\n",
    "      data.append(' '.join(dfText[column2][i]))\n",
    "\n",
    "   data = list(set(data))\n",
    "   \n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_docs(dfText, column1, column2):\n",
    "\n",
    "   data = []\n",
    "   for i in range(0, len(dfText)):\n",
    "      data.append(' '.join(dfText[column1][i]))\n",
    "      data.append(' '.join(dfText[column2][i]))\n",
    "   \n",
    "   return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard(word_tokens1, word_tokens2):\n",
    "\t# Combine both tokens to find union.\n",
    "\tboth_tokens = word_tokens1 + word_tokens2\n",
    "\tunion = set(both_tokens)\n",
    "\n",
    "\t# Calculate intersection.\n",
    "\tintersection = set()\n",
    "\tfor w in word_tokens1:\n",
    "\t\tif w in word_tokens2:\n",
    "\t\t\tintersection.add(w)\n",
    "\n",
    "\tif len(union) == 0:\n",
    "\t\tjaccard_score = 0\n",
    "\telse:\n",
    "\t\tjaccard_score = len(intersection)/len(union)\n",
    "\treturn jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['jaccard'] = dfText.apply(lambda row: calculate_jaccard(row['msg_lemmatized1'], row['msg_lemmatized2']), axis=1)\n",
    "#calculate_jaccard(dfText['msg_lemmatized1'][0], dfText['msg_lemmatized2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['jaccard', 'Overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['jaccard', 'Overall']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(doc1, doc2):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    text_list1 = ' '.join(doc1)\n",
    "    text_list2 = ' '.join(doc2)\n",
    "    \n",
    "    text_list = [text_list1, text_list2]\n",
    "\n",
    "    vector = vectorizer.fit_transform(text_list)\n",
    "    \n",
    "    cosine_similarities = cosine_similarity(vector[0], vector[1])#.flatten()\n",
    "    \n",
    "    return cosine_similarities[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['bow'] = dfText.apply(lambda row: get_bow(row['no_stopwords1'], row['no_stopwords2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['bow', 'Overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['bow', 'Overall']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(dfText, column1, column2):\n",
    "    \n",
    "    #data = join_unique_docs(dfText, column1, column2)\n",
    "    data = join_docs(dfText, column1, column2)\n",
    "      \n",
    "    tfidf = TfidfVectorizer().fit_transform(data)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(tfidf, index):\n",
    "    \n",
    "    index1 = 2*index\n",
    "    index2 = 2*index + 1\n",
    "    \n",
    "    cosine_similarities = cosine_similarity(tfidf[index1], tfidf[index2])#.flatten()\n",
    "    \n",
    "    return cosine_similarities[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = 'no_stopwords1'\n",
    "column2 = 'no_stopwords2'\n",
    "\n",
    "tfidf = calculate_tfidf(dfText, column1, column2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfText['tfidf'] = dfText.apply(lambda row: get_tfidf(tfidf, row.index), axis=1)\n",
    "\n",
    "tfidf_list = []\n",
    "for i in range(len(dfText)):\n",
    "    tfidf_list.append(get_tfidf(tfidf, i))\n",
    "    \n",
    "dfText['tfidf'] = tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['tfidf', 'Overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['tfidf', 'Overall']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec (https://medium.com/red-buffer/doc2vec-computing-similarity-between-the-documents-47daf6c828cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for i in range(0,len(dfText)):\n",
    "#     data.append(' '.join(dfText['msg_lemmatized1'][i]))\n",
    "#     data.append(' '.join(dfText['msg_lemmatized2'][i]))\n",
    "\n",
    "# data = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=30, min_count=0, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_cos(doc1, doc2):\n",
    "    infer1 = model.infer_vector(doc1)\n",
    "    infer2 = model.infer_vector(doc2)\n",
    "    #cos_distance = spatial.distance.cosine(infer1, infer2) #pode ser >1\n",
    "    cos_similarity = 1-spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    return cos_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['doc2vec'] = dfText.apply(lambda row: doc2vec_cos(row['msg_tokenized1'], row['msg_tokenized2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['doc2vec', 'Overall']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Proximos passos: definir pipeline, aplicar o metodo para no_stopwords, msg_lemmatized e msg_stemmed.\n",
    "### Para verificar qual é o melhor método podemos colocar o Overall na mesma escala (0 a 1?) e comparar utilizando\n",
    "### Alguma métrica\n",
    "## obs: ver tbm se esta utilizando CBOW ou..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfText[dfText['doc2vec']<0][['Overall', 'jaccard', 'doc2vec', 'pair_id']]\n",
    "#dfText[['Overall', 'jaccard', 'doc2vec', 'pair_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.hist('Overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting scatterplot overall vs doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(data=dfText, x=\"Overall\", y=\"doc2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Inicializando o modelo\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codificando as sentencas --> Transformando para espaço vetorial\n",
    "# sentence_embeddings = model.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_cos(doc1, doc2):\n",
    "    data = [doc1,doc2]\n",
    "    sentence_embeddings = model.encode(data)\n",
    "\n",
    "    infer1 = sentence_embeddings[0]\n",
    "    infer2 =  sentence_embeddings[1]\n",
    "    #cos_distance = spatial.distance.cosine(infer1, infer2) #pode ser >1\n",
    "    cos_similarity = 1-spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    return cos_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['bert'] = dfText.apply(lambda row: bert_cos(\" \".join(row['no_stopwords1']), \" \".join(row['no_stopwords2'])), axis=1)\n",
    "\n",
    "# dfText['bert2'] = dfText.iloc[:200].apply(lambda row: bert_cos(\" \".join(row['no_stopwords1']), \" \".join(row['no_stopwords2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[['bert', 'Overall']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[dfText['Overall']<1.5][['bert', 'Overall']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[[\"Overall\", \"bert\",\"doc2vec\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dfText, x=\"Overall\", y=\"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# print(api.load(\"word2vec-google-news-300\", return_path=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "#model = KeyedVectors.load_word2vec_format('data/wiki.en.vec', binary=False)\n",
    "# model = KeyedVectors.load_word2vec_format('model/word2vec-google-news-300.gz', binary=True)\n",
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(word2vec_model, words): #words eh um documento inteiro\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in model.index_to_key]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model[words], axis=0)\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_vector(model, dfText['no_stopwords1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_cos(doc1, doc2, model):\n",
    "    mean_doc1 = get_mean_vector(model, doc1)\n",
    "    mean_doc2 = get_mean_vector(model, doc2)\n",
    "\n",
    "    infer1 = mean_doc1\n",
    "    infer2 =  mean_doc2\n",
    "    #cos_distance = spatial.distance.cosine(infer1, infer2) #pode ser >1\n",
    "    cos_similarity = 1-spatial.distance.cosine(infer1, infer2) #de 0 a 1\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText['word2vec_mean'] = dfText.apply(lambda row: bert_cos(\" \".join(row['no_stopwords1']), \" \".join(row['no_stopwords2']), model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    vec = get_mean_vector(model, doc.words)\n",
    "    if len(vec) > 0:\n",
    "      # do somthing with the vector ${vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "class MyCorpus():\n",
    "    def __init__(self, train_data):\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def __iter__(self):\n",
    "        p = PorterStemmer()\n",
    "        for i in range(len(self.train_data)):\n",
    "            doc = self.train_data['text'][i]\n",
    "            doc = re.sub(r'\\S*@\\S*\\s?', '', doc, flags=re.MULTILINE) # remove email\n",
    "            doc = re.sub(r'http\\S+', '', doc, flags=re.MULTILINE) # remove web addresses\n",
    "            doc = re.sub(\"\\'\", \"\", doc) # remove single quotes\n",
    "            doc = remove_stopwords(doc)\n",
    "            doc = p.stem_sentence(doc)\n",
    "            words = simple_preprocess(doc, deacc=True)\n",
    "            yield TaggedDocument(words=words, tags=[self.train_data['documentId'][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapear resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def transformarResultado(resultado, OldMax=-1, OldMin=1, NewMax=4, NewMin=1):\n",
    "    OldRange = (OldMax - OldMin)  \n",
    "    NewRange = (NewMax - NewMin)  \n",
    "    NewValue = (((resultado - OldMin) * NewRange) / OldRange) + NewMin\n",
    "    return NewValue\n",
    "\n",
    "    \n",
    "dfText['bert_norm'] = dfText['bert'].apply(lambda x: transformarResultado(x))\n",
    "dfText['bert_norm1'] = dfText['bert'].apply(lambda x: transformarResultado(x, OldMax=min(dfText['bert']), OldMin = max(dfText['bert'])))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfText[['bert', 'bert_norm', 'bert_norm1', 'Overall']].head(20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578d23e9265697bad3ff07bcff4c72684a1087c3ecac6cf42decc0cb53b76ed7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('data_science': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
